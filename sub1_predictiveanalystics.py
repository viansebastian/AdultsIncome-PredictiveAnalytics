# -*- coding: utf-8 -*-
"""Sub1_PredictiveAnalystics.ipynb

Automatically generated by Colaboratory.

# ***Laporan Proyek Machine Learning: Predictive Analytics On Adults Income - Vian Sebastian Bromokusumo***

### ***Domain Proyek***

---

**Latar Belakang**


Menurut survei oleh Badan Pusat Statistik Indonesia yang dipublikasikan pada 6 November 2023, tingkat pengangguran di Indonesia adalah sebesar 5,32 persen [[1]](https://www.bps.go.id/id/pressrelease/2023/11/06/2002/tingkat-pengangguran-terbuka--tpt--sebesar-5-32-persen-dan-rata-rata-upah-buruh-sebesar-3-18-juta-rupiah-per-bulan.html). Meskipun hal ini merupakan penurunan dari tahun-tahun sebelumnya, hal ini tetap menjadi isu yang perlu terus diselesaikan. Hasil penelitian Pramudjasi dan Juliansyah, 2019 dalam Jurnal FEB Unmul [[2]](https://journal.feb.unmul.ac.id/index.php/KINERJA/article/download/5284/472) menyatakan bahwa jumlah penduduk berpengaruh positif terhadap tingkat pengangguran secara signifikan. Hal ini dikuatkan pula dalam penelitian Sari dan Pangestuty , 2022 dalam  [[3]](https://jdess.ub.ac.id/index.php/jdess/article/download/78/57/373) menyatakan bahwa pertambahan jumlah penduduk berdampak pada kenaikan Tingkat Pengangguran Terbuka. Berbagai survei dan penemuan dari jurnal akademik ini menjadi latar belakang Penulis dalam mencoba menganalisa faktor-faktor apa saja yang memiliki keterkaitan kuat terhadap pengangguran, pekerjaan, terutama faktor yang memiliki keterkaitan terhadap pendapatan yang tinggi.

Proyek ini difokuskan pada analisis dataset "Adults Income" dengan tujuan untuk mengidentifikasi dan memprediksi faktor-faktor yang memiliki korelasi tinggi, atau bahkan berkontribusi terhadap pekerjaan terutama jumalah pendapatan seseorang. Dalam hal ini, target fitur dari dataset ini adalah apakah pendapatan per tahun seseorang <50.000 dolar atau >= 50.000 dolar per tahun, sehingga proyek ini akan menyelesaikan kasus klasifikasi. Meskipun dataset ini diambil dari data sensus 1996, dataset ini masih memiliki relevansi yang signifikan di masa kini, karena sifat umum dari variabel-variabel di dataset ini dan tidak adanya perubahan yang drastis pada komponen-komponen penentu penghasilan.

Menurut Jepchumba dari Microsoft [[4]](https://techcommunity.microsoft.com/t5/educator-developer-blog/getting-started-with-using-visual-machine-learning-tools-for/ba-p/3578397), *machine learning* merupakan teknik yang menggunakan matematika tingkat tinggi dan ilmu statistika untuk mengenali pola pada data yang tidak ada secara eksplisit, dan dapat memprediksi sesuai dengan hasil pola tersebut. Dengan beragamnya faktor-faktor (variabel) yang terlibat dalam proyek ini, machine learning menjadi solusi yang terbaik. Identifikasi dan prediksi faktor-faktor akan dilakukan dengan cara mengaplikasikan teknik-teknik *data analysis* seperti *Exploratory Data Analysis* (EDA)  dan menggunakan algoritma-algoritma *Machine Learning* seperti *Random Forest, K-Nearest Neighbors,* dan *Boosting*.

Proyek ini menjadi sarana kecil untuk membantu menyelesaikan masalah sulitnya mencari kerja, dengan menganalisis dinamika faktor-faktor pendapatan per tahun yang tinggi. Hasil dari proyek ini diharapkan dapat membantu Pemerintah dan individu-individu di usia produktif sebagai tambahan *insight* terkait masalah ini, dan membantu pihak-pihak tersebut untuk mengembangkan masyarakat dan diri sendiri untuk peningkatan kualitas hidup.

### ***Business Understanding***

---
Stakeholder dan sasaran:
1. Pemerintah
Sebagai organisasi tingkat tertinggi di sebuah negara, pemerintah dapat membuat kebijakan-kebijakan dan perubahan yang baik, guna meningkatkan kualitas hidup rakyatnya. Salah satu caranya ialah membuat/memperbaiki sistem di negaranya untuk mendorong kemajuan sumber daya manusianya.
2. Individu
Pada tingkat individu, diharapkan hasil proyek ini dapat memberikan insight terhadap faktor-faktor penting yang dapat meningkatkan kualitas hidupnya, melalui pekerjaan dan pekerjaan dengan pendapatan per tahun yang tinggi.

**Problem Statements**
1. Dari berbagai fitur, apa yang paling berpengaruh terhadap income (pendapatan)?
2. Dengan karakteristik tertentu, apakah income dapat diprediksi?  

*income merujuk pada pendapatan per tahun.

**Predictive Modelling Goals**
1. Mengetahui fitur-fitur yang memiliki kaitan yang tinggi terhadap income.
2. Dapat memprediksi income dengan akurasi di atas 90%.

**Solution Statements (Metodologi)**
1. Target feature pada dataset ini merupakan variable boolean antara >50k dan <= 50k, sehingga kasus ini merupakan kasus prediksi Klasifikasi.

2. Model/algoritma yang akan digunakan untuk menyelesaikan kasus ini adalah:
- K-Nearest Neighbor
- Random Forest
- XGBoost
- AdaBoost

3. Dataset ini memiliki fitur tingkat pendidikan, yang mana bersifat ordinal, sehingga Penulis juga ingin menguji keterkaitan ordinalitas data, menggunakan metode *Ordinal Encoding vs One Hot Encoding*.

4. Penulis juga ingin menguji dampak missing values handling antara *KNN Imputation vs Drop* pada akurasi model.

5. Metrik Evaluasi yang akan digunakan pada proyek ini adalah:
- Accuracy
- Precision
- Recall
- F1-Score
- Confusion Matrix, sebagai visualisasi

### ***Data Understanding***

---

Dataset: https://archive.ics.uci.edu/dataset/2/adult
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Dicoding Machine Learning Implementation/archive (4)/adult.csv')
df

df.info()

"""Sejauh ini, kita dapat menarik kesimpulan bahwa terdapat:
1. 9 object columns.
2. 6 integer columns.
3. Jumlah sampel 48.842 data.
4. Jumlah fitur 14 kolom, dan target 1 kolom.
5. Missing values ditandai dengan "?"

---


"""

df.describe()

df.isnull().sum()

df.isna().sum()

missing_values_check = df.columns[df.eq('?').any()]

print(missing_values_check)

"""Dari sini dapat kita konfirmasikan bahwa missing values ditandai dengan '?'. Pada umumnya, missing values dapat juga berupa NaN atau Null.

---


"""

df.rename(columns = {
    'education' : 'educationClass',
    'educational-num' : 'educationLevel',
    'marital-status' : 'status',
    'capital-gain' : 'gain',
    'capital-loss' : 'loss',
    'hours-per-week' : 'hours',
    'native-country' : 'native' }, inplace = True)

df

workclass_missing = (df.workclass == '?').sum()
occ_missing = (df.occupation == '?').sum()
native_missing = (df.native == '?').sum()

print(f"workclass missing values = {workclass_missing}")
print(f"occupation missing values = {occ_missing}")
print(f"native missing values = {native_missing}")

"""Terdapat beberapa metode yang dapat dilakukan terkait missing values ini. Dikarenakan rasio dari missing values terhadap total sampel dapat terbilang cukup kecil, dengan worst case scenario 1:7, metode Drop masih sangat baik dilakukan.

---


"""

# map target to binary classification
df['income'] = df['income'].map({'<=50K': 0, '>50K': 1})

# drop unnecesarry columns
df.drop(columns = 'fnlwgt', inplace = True, axis = 1)

"""***EDA - Univariate Analysis***

---


"""

obj_cols = df.select_dtypes(include = ['object'])

feature = obj_cols.columns[0]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize = True)
df1 = pd.DataFrame({'sample count': count, 'percentage': percent.round(1)})

print(df1)
count.plot(kind = 'bar', title = feature)

feature = obj_cols.columns[1]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize = True)
df1 = pd.DataFrame({'sample count': count, 'percentage': percent.round(1)})

print(df1)
count.plot(kind = 'bar', title = feature)

feature = obj_cols.columns[2]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize = True)
df1 = pd.DataFrame({'sample count': count, 'percentage': percent.round(1)})

print(df1)
count.plot(kind = 'bar', title = feature)

feature = obj_cols.columns[3]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize = True)
df1 = pd.DataFrame({'sample count': count, 'percentage': percent.round(1)})

print(df1)
count.plot(kind = 'bar', title = feature)

replacement_dict = {
    'Prof-specialty': 'Specialty',
    'Craft-repair': 'Repair',
    'Exec-managerial': 'Managerial',
    'Adm-clerical': 'Clerical',
    'Other-service': 'Others',
    'Machine-op-inspct': 'Inspection',
    'Transport-moving': 'Transport',
    'Handlers-cleaners': 'Cleaners',
    'Farming-fishing': 'Agriculture',
    'Tech-support': 'ITSup',
    'Protective-serv': 'ProtectiveService',
    'Priv-house-serv': 'PrivateHouse Service',
    'Armed-Forces': 'ArmedForces'
}

df['occupation'] = df['occupation'].replace(replacement_dict)

feature = obj_cols.columns[4]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize = True)
df1 = pd.DataFrame({'sample count': count, 'percentage': percent.round(1)})

print(df1)
count.plot(kind = 'bar', title = feature)

feature = obj_cols.columns[5]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize = True)
df1 = pd.DataFrame({'sample count': count, 'percentage': percent.round(1)})

print(df1)
count.plot(kind = 'bar', title = feature)

feature = obj_cols.columns[6]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize = True)
df1 = pd.DataFrame({'sample count': count, 'percentage': percent.round(1)})

print(df1)
count.plot(kind = 'bar', title = feature)

replacement_dict = {
      'United-States' : 'USA',
      'Puerto-Rico' : 'PuertoRic',
      'El-Savador' : 'ElSavad',
      'Dominican-Republic' : 'Dominican',
      'Trinadad&Tobago' : 'Trinadad',
      'Outlying-US(Guam-USVI-etc)' : 'USBor',
      'Holand-Netherlands' : 'Holland'
}

df['native'] = df['native'].replace(replacement_dict)

feature = obj_cols.columns[7]
count = df[feature].value_counts()
percent = 100 * df[feature].value_counts(normalize = True)
df1 = pd.DataFrame({'sample count': count, 'percentage': percent.round(1)})

print(df1)
count.plot(kind = 'bar', title = feature)

df.hist(bins = 50, figsize = (20,15))
plt.show()

"""Dari hasil Univariate Analysis, dapat dilihat bahwa dataset memiliki kecenderungan yang tinggi pada perbedaan modus dan data yang lain yang sangat besar. Hal ini menyiratkan bahwa dataset ini dapat juga diatasi missing valuesnya menggunakan metode statistik, seperti *mode imputation*.

***EDA - Multivariate Analysis***

---
"""

obj_list = obj_cols.columns.to_list()

plt.figure(figsize = (15, 8))

for col in obj_list:
  # plt.subplot(1, len(obj_list), i + 1)
  sns.catplot(x = col, y = 'income', kind = 'bar', dodge = False, height = 4, aspect = 3,
              data = df, palette = 'Set3')
  plt.xticks(rotation = 45)
  plt.title("Average 'income' Relative to {}".format(col))

plt.tight_layout()
plt.show()

"""Hasil dari Multivariate Analysis pada Categorical Features adalah sebagai berikut.

- Pada workclass, dapat dilihat bahwa semua workclass memiliki income, kecuali tentunya never-worked. Rata-rata yang bekerja pada pemerintah (-gov) memiliki income yang lebih tinggi dibandingkan dengan workclass lain, namun self-emp-inc memiliki pendapatan paling tinggi.

- Pada educationClass, dapat dilihat dengan jelas bahwa terdapat kesenjangan yang sangat tinggi mulai dari tingkat pendidikan S1 (bachelors), dan lebih tinggi lagi semakin tinggi gelar yang dimiliki (masters, doctorate, prof-school).

- Pada status pernikahan, dapat dilihat pula tren bahwa data dengan status pernikahan stabil memiliki pendapatan yang lebih tinggi dibandingkan mereka yang tidak menikah, bercerai, berpisah, atau cerai mati.

- Pada occupation, tiga sektor kerja paling tinggi pendapatannya adalah Specialty (spesialis), manajerial (eksekutif), dan protective service (jasa keamanan). Selain itu, tidak terdapat tren yang secara jelas dapat di diamati.

- Pada relationship, dapat dilihat bahwa menguatkan hasil pengamatan status pernikahan, yang berstatus sebagai husband (suami) dan istri (istri) memiliki pendapatan yang tertinggi dari data-data lain.

- Pada race, dua ras dengan pendapatan tertinggi adalah White (putih) dan Asian-Pac (asia-pasifik).

- Pada gender, dapat dilihat bahwa pendapatan laki-laki lebih tinggi dari pendapatan perempuan.

- pada native (negara asal/buyut), dapat dilihat bahwa tren pendapatan kurang dapat diamati, artinya negara asal/buyut bukan faktor yang kuat yang dapat memengaruhi pendapatan.

Kesimpulannya, dari fitur kategorikal, fitur yang memiliki pengaruh tinggi adalah educationClass, status, dan relationship.

---


"""

# Numerical Features
sns.pairplot(df, diag_kind = 'kde')

!pip install dython

# Correlation Matrix
from dython.nominal import associations

complete_correlation = associations(df, filename='complete_correlation.png', figsize=(10, 10))

df_complete_corr = complete_correlation['corr']
df_complete_corr = df_complete_corr.dropna(axis=1, how='all').dropna(axis=0, how='all')


styled_corr_matrix = df_complete_corr.style.background_gradient(cmap='coolwarm', axis=None)

"""Menguatkan pengamatan dari *Multivariate Analysis* pada fitur kategorikal, dapat dilihat bahwa *status* dan *relationship* memiliki nilai korelasi paling kuat terhadap *income*. Namun dikarenakan *range* antara nilai korelasi paling kuat ke lemah tidak terlalu jauh, tidak ada fitur yang akan di drop.

---


"""

# Drop rows yang memiliki missing values
df_drop = df.loc[(df[['workclass','occupation','native']] != '?').all(axis=1)]
df_drop

"""### ***Data Preparation***

---
Secara berurutan, proses Data Preparation yang akan dilewati adalah
1. Features Encoding
2. Train Test Split
3. KNN Imputation
4. Standardization

Pada tahap Data Preparation ini, akan dilakukan eksplorasi tambahan, antara lain dampak ordinality dan metode missing values handling pada data dan akurasi nantinya. Data yang akan kita tinjau adalah
- Dropped Data:
1. Dropped Dummy Data (One Hot Encoded)
2. Dropped Ordinal Encoded Data (Ordinal Encoded)

- KNN Imputed data:
1. KNN Imputed Ordinal Encoded Data,
dimana, imputasi akan dilakukan setelah proses Train Test Split, untuk mencegah terjadinya Data Leakage.

Pertama-tama, akan dilakukan Features Encoding. Disini akan akan diterapkan One Hot Encoding dan Ordinal Encoding. Hal ini untuk mengamati pengaruh dari metode Encoding yang berbeda pada pengawetan sifat ordinal, yang akan mempengaruhi hasil akhir akurasi model. Sebelum Data Preparation, sudah ada satu dataframe yang merupakan copy dari data asli, dan proses Drop sudah dilakukan. Output dari tahap ini adalah tiga dataframe, yaitu data drop One Hot Encoded, data drop Ordinal encoded dan data asli Ordinal Encoded.

Kedua, akan dilakukan Train Test Split. Disini akan dilakukan proses train test split. 3 dataFrame akan displit menjadi 6, masing-masing x dan y.

Kemudian akan dilakukan KNN Imputation pada data asli Ordinal Encoded. KNN Imputation dilakukan disini karena potensi Data Leakage, sehingga harus dianggap sebagai proses transformasi.

Terakhir, proses Standarisasi dilakukan pada semua x_train dan x_test pada semua dataframe, untuk meningkatkan performa model.

Pentingnya [Standarisasi](https://developers.google.com/machine-learning/data-prep/transform/normalization#:~:text=The%20goal%20of%20normalization%20is,training%20stability%20of%20the%20model)

---

***Dropped Data Preparation***
"""

# import Ordinal Encoding library
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
df_drop_ordinal = df_drop.copy()
df_drop_dum = df_drop.copy()

# Ordinal Encoded
df_drop_ordinal[obj_cols.columns] = ordinal_encoder.fit_transform(df_drop_ordinal[obj_cols.columns])

# Dummy Encoded
df_drop_dum = pd.get_dummies(df_drop_dum, columns = obj_cols.columns, drop_first = True)

df_drop_ordinal

df_drop_dum

# data asli/initial
df

ordinal_encoder = OrdinalEncoder()
df.replace('?', np.nan, inplace = True)

df[obj_cols.columns] = ordinal_encoder.fit_transform(df[obj_cols.columns])

df.isna().sum()

# train test split
from sklearn.model_selection import train_test_split

# Dropped Dummy Encoded data
x_drop_dum = df_drop_dum.drop(['income'], axis = 1)
y_drop_dum = df_drop_dum['income']

# Dropped Ordinal Encoded data
x_drop_ordinal = df_drop_ordinal.drop(['income'], axis = 1)
y_drop_ordinal = df_drop_ordinal['income']

# Original data, to be KNN Imputed
x_kimp = df.drop(['income'], axis = 1)
y_kimp = df['income']

# train test split
x_train_dd, x_test_dd, y_train_dd, y_test_dd = train_test_split(x_drop_dum, y_drop_dum, test_size = 0.1, random_state = 123)
x_train_do, x_test_do, y_train_do, y_test_do = train_test_split(x_drop_ordinal, y_drop_ordinal, test_size = 0.1, random_state = 123)
x_train_kimp, x_test_kimp, y_train_kimp, y_test_kimp = train_test_split(x_kimp, y_kimp, test_size = 0.1, random_state = 123)

# print results
print(f"Total sample in dataset (dum): {len(x_drop_dum)}")
print(f"Total sample in dataset (do): {len(x_drop_ordinal)}")
print(f"Total sample in dataset (kimp): {len(x_kimp)}")
print(f"Total train samples (dum): {len(x_train_dd)}")
print(f"Total train samples (do): {len(x_train_do)}")
print(f"Total train samples (kimp): {len(x_train_kimp)}")
print(f"Total test samples (do): {len(x_test_do)}")
print(f"Total test samples (dum): {len(x_test_dd)}")
print(f"Total test samples (kimp): {len(x_test_kimp)}")

"""***KNN Imputation Data Preparation***

---

KNN Imputation dilakukan pada proses Data Preparation, setelah train test split, dikarenakan metode ini memiliki potensi Data Leakage, sehingga perlu dianggap sebagai tahap transformasi. Sama seperti proses Data Preparation pada Dropped Data, akan dilakukan Features Encoding terlebih dahulu, dilanjutkan ke standarisasi.


"""

# check initial train missing values
missing_values_check1 = x_train_kimp.columns[x_train_kimp.eq(np.nan).any()]

print(missing_values_check1)

# check initial test missing values
missing_values_check2 = x_test_kimp.columns[x_test_kimp.eq(np.nan).any()]

print(missing_values_check2)

from sklearn.impute import KNNImputer

# ordinal_encoder = OrdinalEncoder()
# x_train_kimp.replace('?', np.nan, inplace = True)

# x_train_kimp[obj_cols.columns] = ordinal_encoder.fit_transform(x_train_kimp[obj_cols.columns])

imputer = KNNImputer(n_neighbors = 6)

x_train_kimp_imputed = imputer.fit_transform(x_train_kimp)
x_train_kimp_imputed= pd.DataFrame(x_train_kimp_imputed, columns = x_train_kimp.columns)

x_train_kimp_imputed

x_train_kimp_imputed.isna().sum()

missing_values_check_after = x_train_kimp_imputed.columns[x_train_kimp_imputed.eq('?').any()]

print(f"after - columns with missing values: {missing_values_check_after}")

"""Seperti yang terlihat, data train sudah berhasil di imputasi.

---


"""

# x_test_kimp.replace('?', np.nan, inplace = True)

# x_test_kimp[obj_cols.columns] = ordinal_encoder.fit_transform(x_test_kimp[obj_cols.columns])

imputer = KNNImputer(n_neighbors = 6)

x_test_kimp_imputed = imputer.fit_transform(x_test_kimp)
x_test_kimp_imputed= pd.DataFrame(x_test_kimp_imputed, columns = x_test_kimp.columns)

x_test_kimp_imputed

x_test_kimp_imputed.isna().sum()

"""Sama halnya dengan data train, data test sudah berhasil di imputasi.

---


"""

x_train_do

"""***Standarisasi***

---


"""

# Standardization for dummy dataset
from sklearn.preprocessing import StandardScaler

numerical_values_dum = ['age', 'educationLevel', 'gain', 'loss', 'hours']
scaler = StandardScaler()
scaler.fit(x_train_dd[numerical_values_dum])
x_train_dd[numerical_values_dum] = scaler.transform(x_train_dd.loc[:, numerical_values_dum])
x_train_dd[numerical_values_dum].head()

# Standardize dummy dataset test set

x_test_dd.loc[:, numerical_values_dum] = scaler.transform(x_test_dd[numerical_values_dum])
x_test_dd

# Standardize ordinal-encoded data

numerical_values_ord = x_train_do.select_dtypes(include = ['number']).columns # ['age'	, 'workclass',	'educationClass',	'educationLevel',	'status', 'occupation',	'relationship',	'race',	'gender',	'gain', 'loss', 'hours',	'native']
scaler = StandardScaler()
scaler.fit(x_train_do[numerical_values_ord])
x_train_do[numerical_values_ord] = scaler.transform(x_train_do.loc[:, numerical_values_ord])
x_train_do[numerical_values_ord].head()

# Standardize ordinal-encoded test set

x_test_do.loc[:, numerical_values_ord] = scaler.transform(x_test_do[numerical_values_ord])
x_test_do

# Standardize KNN Imputed train data

numerical_values_kimp = x_train_kimp_imputed.select_dtypes(include = ['number']).columns # ['age'	, 'workclass',	'educationClass',	'educationLevel',	'status', 'occupation',	'relationship',	'race',	'gender',	'gain', 'loss', 'hours',	'native']
scaler = StandardScaler()
scaler.fit(x_train_kimp_imputed[numerical_values_ord])
x_train_kimp_imputed[numerical_values_kimp] = scaler.transform(x_train_kimp_imputed.loc[:, numerical_values_kimp])
x_train_kimp_imputed[numerical_values_kimp].head()

# Standardize test data

x_test_kimp_imputed.loc[:, numerical_values_kimp] = scaler.transform(x_test_kimp_imputed[numerical_values_kimp])
x_test_kimp_imputed

print(x_train_kimp_imputed.shape)
print(x_test_kimp_imputed.shape)

"""Dengan ini, data sudah siap digunakan model.

### ***Model Development***

---

Proses Model Development yang akan Penulis lakukan dapat dibagi menjadi beberapa tahap:
1. DataFrame initialization: menyiapkan dataframe untuk menyimpan hasil training model.

2. Pipeline Prep: menyiapkan pipeline untuk mempermudah proses training.

3. Model Training: melatih model.

4. Visualization

***DataFrame Initialization***


---
"""

# initialize dataframe for models

# dataframe for Dropped - OneHotEncoded data
models_dum = pd.DataFrame(
    columns = ['train_acc', 'train_precision', 'train_recall', 'train_f1Score'],
    index = ['KNN', 'RandomForest', 'XGBoost', 'AdaBoost'])

# dataframe for Dropped - OrdinalEncoded data
models_ord = pd.DataFrame(
    columns = ['train_acc', 'train_precision', 'train_recall', 'train_f1Score'],
    index = ['KNN', 'RandomForest', 'XGBoost', 'AdaBoost'])

# dataframe for KNN Imputed data
models_kimp = pd.DataFrame(
    columns = ['train_acc', 'train_precision', 'train_recall', 'train_f1Score'],
    index = ['KNN', 'RandomForest', 'XGBoost', 'AdaBoost'])

print(models_dum)
print(models_ord)
print(models_kimp)

"""***Training Pipeline Prep***

---


"""

# pipelines

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# pipeline train function
def train(classifier, x_train, y_train, model_name, models_df):
  classifier.fit(x_train, y_train)

  pred_train = classifier.predict(x_train)

  acc_train = accuracy_score(y_true = y_train, y_pred = pred_train)
  precision_train = precision_score(y_true = y_train, y_pred = pred_train)
  recall_train = recall_score(y_true = y_train, y_pred = pred_train)
  f1score_train = f1_score(y_true = y_train, y_pred = pred_train)

  models_df.loc[model_name, 'train_acc'] = acc_train
  models_df.loc[model_name, 'train_precision'] = precision_train
  models_df.loc[model_name, 'train_recall'] = recall_train
  models_df.loc[model_name, 'train_f1Score'] = f1score_train

  return models_df

"""***Model Training***

---


"""

# model training
forest = RandomForestClassifier(n_estimators = 50, max_depth = 16, random_state = 55, n_jobs = -1)
models_dum = train(forest, x_train_dd, y_train_dd, 'RandomForest', models_dum)
models_ord = train(forest, x_train_do, y_train_do, 'RandomForest', models_ord)
models_kimp = train(forest, x_train_kimp_imputed, y_train_kimp, 'RandomForest', models_kimp)

knn = KNeighborsClassifier(n_neighbors = 10)
models_dum = train(knn, x_train_dd, y_train_dd, 'KNN', models_dum)
models_ord = train(knn, x_train_do, y_train_do, 'KNN', models_ord)
models_kimp = train(knn, x_train_kimp_imputed, y_train_kimp, 'KNN', models_kimp)

xgb = XGBClassifier()
models_dum = train(xgb, x_train_dd, y_train_dd, 'XGBoost', models_dum)
models_ord = train(xgb, x_train_do, y_train_do, 'XGBoost', models_ord)
models_kimp = train(xgb, x_train_kimp_imputed, y_train_kimp, 'XGBoost', models_kimp)

ada = AdaBoostClassifier(n_estimators = 50, random_state = 123)
models_dum = train(ada, x_train_dd, y_train_dd, 'AdaBoost', models_dum)
models_ord = train(ada, x_train_do, y_train_do, 'AdaBoost', models_ord)
models_kimp = train(ada, x_train_kimp_imputed, y_train_kimp, 'AdaBoost', models_kimp)

"""***Model Explanation, pros and cons***

---

*dijelaskan di laporan proyek lengkap (.md)
"""

models_dum

models_ord

models_kimp

"""***Visualization***

---


"""

fig, ax = plt.subplots(nrows = 3, figsize = (10, 15))

models_dum.sort_values(by = 'train_acc', ascending = False).plot(kind = 'barh', ax = ax[0], zorder = 3)
ax[0].set_ylabel('Models')
ax[0].set_title('Train Accuracy - models_dum')
ax[0].grid(zorder = 0)

models_ord.sort_values(by = 'train_acc', ascending = False).plot(kind = 'barh', ax = ax[1], zorder = 3)
ax[1].set_ylabel('Models')
ax[1].set_title('Train Accuracy - models_ord')
ax[1].grid(zorder = 0)

models_kimp.sort_values(by = 'train_acc', ascending = False).plot(kind = 'barh', ax = ax[2], zorder = 3)
ax[2].set_ylabel('Models')
ax[2].set_title('Train Accuracy - models_kimp')
ax[2].grid(zorder = 0)

plt.tight_layout()
plt.show()

"""Dilihat dari hasil di atas, dapat disimpulkan bahwa ordinalitas data penting (jika ada) untuk disimpan menggunakan Ordinal Encoding atau sejenisnya. Sejauh ini, hasil yang paling baik adalah Random Forest pada data KNN Imputed Ordinal Encoded. Namun untuk evaluasinya tetap akan sama seperti training, hanya saja menggunakan test data.

### ***Evaluation***

---

Dalam proyek ini, beberapa metrik evaluasi yang digunakan adalah sebagai berikut.

Sebelum memasuki penjelasan metrik lebih lanjut, perlu dipahami bahwa:

- TN = True Negative, data negatif yang diprediksi negatif (benar)
- TP = True Positive, data positif yang diprediksi negatif (benar)
- FN = False Negative, data negatif yang diprediksi positif (salah)
- FP = False positive, data positif yang diprediksi negatif (salah)



---


Pertama, Accuracy, yang dapat dihitung dengan rumus:
$$
Accuracy = \frac{TN + TP}{TN + FP + TP + FN}
$$

Accuracy merepresentasikan angka data yang benar di prediksi dibagi total jumlah data. Idealnya, akurasi memberikan ide seberapa baik model dapat memprediksi data, namun kekurangan dari metrik ini adalah kurang adilnya metrik jika dataset yang digunakan *unbalanced*.


---


Berikutnya adalah precision, yang dapat dihitung dengan rumus:

$$
Precision = \frac{TP}{FP + TP}
$$

Precision adalah rasio prediksi benar positif (TP) dari total prediksi positif. Semakin tinggi presisi, artinya semakin sedikit jumlah prediksi positif salah (FP).


---


Kemudian terdapat metrik Recall, seperti berikut.

$$
Recall = \frac{TP}{TP + FN}
$$

Recall menghitung nilai dari betulnya prediksi positif dari jumlah aktual positif. Semakin tinggi nilai recall berarti semakin sedikit False Negatives (FN).



---


Setelah menghitung metrik Accuracy, Precision, dan Recall, kita bisa mencari nilai F1 Score dengan rumus berikut.

$$
F1 Score = 2* \frac{Precision * Recall}{Precision + Recall}
$$

F1 Score adalah sebuah nilai harmonis yang menggunkan presisi dan Recall, artinya  nilai F1 Score yang tinggi memiliki Precision dan Recall yang tinggi.

***Initialize DataFrame and Pipeline for Testing***

---
"""

# dataframe for Dropped - OneHotEncoded data
test_dum = pd.DataFrame(
    columns = ['test_acc', 'test_precision', 'test_recall', 'test_f1Score'],
    index = ['KNN', 'RandomForest', 'XGBoost', 'AdaBoost'])

# dataframe for Dropped - OrdinalEncoded data
test_ord = pd.DataFrame(
    columns = ['test_acc', 'test_precision', 'test_recall', 'test_f1Score'],
    index = ['KNN', 'RandomForest', 'XGBoost', 'AdaBoost'])

# dataframe for KNN Imputed data
test_kimp = pd.DataFrame(
    columns = ['test_acc', 'test_precision', 'test_recall', 'test_f1Score'],
    index = ['KNN', 'RandomForest', 'XGBoost', 'AdaBoost'])


# pipeline test function
def test(classifier, x_test, y_test, model_name, models_df):
  classifier.fit(x_test, y_test)

  pred_test = classifier.predict(x_test)

  acc_test = accuracy_score(y_true = y_test, y_pred = pred_test)
  precision_test = precision_score(y_true = y_test, y_pred = pred_test)
  recall_test = recall_score(y_true = y_test, y_pred = pred_test)
  f1score_test = f1_score(y_true = y_test, y_pred = pred_test)

  models_df.loc[model_name, 'test_acc'] = acc_test
  models_df.loc[model_name, 'test_precision'] = precision_test
  models_df.loc[model_name, 'test_recall'] = recall_test
  models_df.loc[model_name, 'test_f1Score'] = f1score_test

  return pred_test, models_df

"""***Model Testing***

---


"""

# model testing

# Random Forest Test
forrest_test_dum, test_dum = test(forest, x_test_dd, y_test_dd, 'RandomForest', test_dum)
forrest_test_ord, test_ord = test(forest, x_test_do, y_test_do, 'RandomForest', test_ord)
forrest_test_kimp, test_kimp = test(forest, x_test_kimp_imputed, y_test_kimp, 'RandomForest', test_kimp)

# KNN Test
knn_test_dum, test_dum = test(knn, x_test_dd, y_test_dd, 'KNN', test_dum)
knn_test_ord, test_ord = test(knn, x_test_do, y_test_do, 'KNN', test_ord)
knn_test_kimp, test_kimp = test(knn, x_test_kimp_imputed, y_test_kimp, 'KNN', test_kimp)

# XGBoost Test
xgb_test_dum, test_dum = test(xgb, x_test_dd, y_test_dd, 'XGBoost', test_dum)
xgb_test_ord, test_ord = test(xgb, x_test_do, y_test_do, 'XGBoost', test_ord)
xgb_test_kimp, test_kimp = test(xgb, x_test_kimp_imputed, y_test_kimp, 'XGBoost', test_kimp)

# AdaBoost Test
ada_test_dum, test_dum = test(ada, x_test_dd, y_test_dd, 'AdaBoost', test_dum)
ada_test_ord, test_ord = test(ada, x_test_do, y_test_do, 'AdaBoost', test_ord)
ada_test_kimp, test_kimp = test(ada, x_test_kimp_imputed, y_test_kimp, 'AdaBoost', test_kimp)

test_dum

test_ord

test_kimp

"""Melihat hasil dari hasil test diatas, terutama dari test_dum dan test_ord, dapat dilihat bahwa terjadi peningkatan akurasi dari dua model dengan akurasi tertinggi, yaitu Random Forest dan XGBoost. Hal ini membenarkan bahwa penting untuk menyimpan fitur ordinality (jika ada) yang dalam hal ini ada pada dataset, terutama educationClass.


Secara konsisten, model Random Forest dan XGBoost menghasilkan akurasi dan F1 Score yang tinggi, sebuah indikasi dari model yang unggul.


Pengujian prediksi terakhir akan menggunakan test data dari test_ord, dikarenakan dataset ini mengimplementasikan Ordinal Encoding dan memiliki tingkat integritas yang lebih tinggi dari dataset imputasi. Mengingat bahwa jumlah dataset ini melimpah dari awal, metode Drop tetap menjadi metode Data Handling terbaik karena pengawetan integritasnya.

***Test Visualization***


---
"""

fig, ax = plt.subplots(nrows = 3, figsize = (10, 15))

test_dum.sort_values(by = 'test_acc', ascending = False).plot(kind ='barh', ax = ax[0], zorder = 3)
ax[0].set_ylabel('test')
ax[0].set_title('Test Accuracy - test_dum')
ax[0].grid(zorder = 0)

test_ord.sort_values(by = 'test_acc', ascending = False).plot(kind = 'barh', ax = ax[1], zorder = 3)
ax[1].set_ylabel('test')
ax[1].set_title('Test Accuracy - test_ord')
ax[1].grid(zorder = 0)

test_kimp.sort_values(by = 'test_acc', ascending = False).plot(kind = 'barh', ax = ax[2], zorder = 3)
ax[2].set_ylabel('test')
ax[2].set_title('Test Accuracy - test_kimp')
ax[2].grid(zorder = 0)

plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix

# confusion matrix builder
def plot_confusion_matrix(model_name, y_true, y_pred, ax):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues', cbar = False, ax = ax)
    ax.set_title(f'Confusion Matrix - {model_name}')
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Actual')


# show the confusion matrix
fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize = (10, 8))

plot_confusion_matrix('RandomForest', y_test_do, forrest_test_ord, ax[0, 0])
plot_confusion_matrix('KNN', y_test_do, knn_test_ord, ax[0, 1])
plot_confusion_matrix('XGBoost', y_test_do, xgb_test_ord, ax[1, 0])
plot_confusion_matrix('AdaBoost', y_test_do, ada_test_ord, ax[1, 1])

plt.tight_layout()
plt.show()

"""Dari confusion matrix ini dapat dilihat lebih jelas bahwa dari segi TN, TP, FN dan FP, ***Random Forest dan XGBoost*** merupakan dua model dengan performa terbaik.

Dengan mendapatkan nilai paling tinggi di semua metrik secara konsisten, maka dapat disimpulkan solusi dan model terbaik untuk problem ini adalah *Random Forest*.

***Test Prediction***

---
"""

# Prediction
model_dict = {'KNN' : knn, 'RandomForest' : forest, 'XGBoost' : xgb, 'AdaBoost' : ada}

prediction = x_test_do.iloc[:10].copy()
pred_dict = {'y_true':y_test_do[:10]}
for name, model in model_dict.items():
    pred_dict['prediction_'+name] = model.predict(prediction).round(1)

pd.DataFrame(pred_dict)

"""
***Kesimpulan Akhir***

---

1. Fitur-fitur yang paling berpengaruh terhadap income pada problem ini adalah educationClass, status, dan relationship.
2. Ya, income dapat diprediksi, dan berdasarkan nilai *Accuracy, Recall, Precision, dan F1 Score*, model Random Forest adalah model dengan performa terbaik, mencapai akurasi hingga 95,4%, presisi hingga 93%, recall 88,3%, dan F1Score 90,6%. Hal ini merupakan salah satu bentuk kelebihan dari Random Forest yaitu sifatnya yang melakukan *feature selection* secara implisit. Ditambah lagi sifat yang diwariskan dari *ensemble learning*, yaitu tahan pada bias dan overfitting.
3. Terdapat pengaruh yang dapat diamati dalam bentuk peningkatan performa model jika pengawetan ordinalitas (jika ada dalam dataset) dilakukan.
4. *KNN Imputation* memiliki dampak yang kurang signifikan jika dibandingkan dengan poin nomor 3, namun performa tetap meningkat.

---

*Untuk selengkaptnya dapat merujuk kembali ke Laporan Proyek lengkap (.md)*
"""