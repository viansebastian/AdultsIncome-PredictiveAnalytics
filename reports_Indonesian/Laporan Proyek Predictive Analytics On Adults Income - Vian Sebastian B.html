<!DOCTYPE html>
<html>
<head>
<title>Laporan Proyek Predictive Analytics On Adults Income - Vian Sebastian B.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="laporan-proyek-machine-learning-predictive-analytics-on-adults-income---vian-sebastian-bromokusumo"><em><strong>Laporan Proyek Machine Learning: Predictive Analytics On Adults Income - Vian Sebastian Bromokusumo</strong></em></h1>
<h2 id="domain-proyek"><em><strong>Domain Proyek</strong></em></h2>
<p><strong>Latar Belakang</strong></p>
<p>Menurut survei oleh Badan Pusat Statistik Indonesia yang dipublikasikan pada 6 November 2023, tingkat pengangguran di Indonesia adalah sebesar 5,32 persen <a href="https://www.bps.go.id/id/pressrelease/2023/11/06/2002/tingkat-pengangguran-terbuka--tpt--sebesar-5-32-persen-dan-rata-rata-upah-buruh-sebesar-3-18-juta-rupiah-per-bulan.html">[1]</a>. Meskipun hal ini merupakan penurunan dari tahun-tahun sebelumnya, hal ini tetap menjadi isu yang perlu terus diselesaikan. Hasil penelitian Pramudjasi dan Juliansyah, 2019 dalam Jurnal FEB Unmul <a href="https://journal.feb.unmul.ac.id/index.php/KINERJA/article/download/5284/472">[2]</a> menyatakan bahwa jumlah penduduk berpengaruh positif terhadap tingkat pengangguran secara signifikan. Hal ini dikuatkan pula dalam penelitian Sari dan Pangestuty , 2022 dalam  <a href="https://jdess.ub.ac.id/index.php/jdess/article/download/78/57/373">[3]</a> menyatakan bahwa pertambahan jumlah penduduk berdampak pada kenaikan Tingkat Pengangguran Terbuka. Berbagai survei dan penemuan dari jurnal akademik ini menjadi latar belakang Penulis dalam mencoba menganalisa faktor-faktor apa saja yang memiliki keterkaitan kuat terhadap pengangguran, pekerjaan, terutama faktor yang memiliki keterkaitan terhadap pendapatan yang tinggi.</p>
<p>Proyek ini difokuskan pada analisis dataset &quot;Adults Income&quot; dengan tujuan untuk mengidentifikasi dan memprediksi faktor-faktor yang memiliki korelasi tinggi, atau bahkan berkontribusi terhadap pekerjaan terutama jumalah pendapatan seseorang. Dalam hal ini, target fitur dari dataset ini adalah apakah pendapatan per tahun seseorang &lt;50.000 dolar atau &gt;= 50.000 dolar per tahun, sehingga proyek ini akan menyelesaikan kasus klasifikasi. Meskipun dataset ini diambil dari data sensus 1996, dataset ini masih memiliki relevansi yang signifikan di masa kini, karena sifat umum dari variabel-variabel di dataset ini dan tidak adanya perubahan yang drastis pada komponen-komponen penentu penghasilan.</p>
<p>Menurut Jepchumba dari Microsoft <a href="https://techcommunity.microsoft.com/t5/educator-developer-blog/getting-started-with-using-visual-machine-learning-tools-for/ba-p/3578397">[4]</a>, <em>machine learning</em> merupakan teknik yang menggunakan matematika tingkat tinggi dan ilmu statistika untuk mengenali pola pada data yang tidak ada secara eksplisit, dan dapat memprediksi sesuai dengan hasil pola tersebut. Dengan beragamnya faktor-faktor (variabel) yang terlibat dalam proyek ini, machine learning menjadi solusi yang terbaik. Identifikasi dan prediksi faktor-faktor akan dilakukan dengan cara mengaplikasikan teknik-teknik <em>data analysis</em> seperti <em>Exploratory Data Analysis</em> (EDA)  dan menggunakan algoritma-algoritma <em>Machine Learning</em> seperti <em>Random Forest, K-Nearest Neighbors,</em> dan <em>Boosting</em>.</p>
<p>Proyek ini menjadi sarana kecil untuk membantu menyelesaikan masalah sulitnya mencari kerja, dengan menganalisis dinamika faktor-faktor pendapatan per tahun yang tinggi. Hasil dari proyek ini diharapkan dapat membantu Pemerintah dan individu-individu di usia produktif sebagai tambahan <em>insight</em> terkait masalah ini, dan membantu pihak-pihak tersebut untuk mengembangkan masyarakat dan diri sendiri untuk peningkatan kualitas hidup.</p>
<h2 id="business-understanding"><em><strong>Business Understanding</strong></em></h2>
<p>Stakeholder dan sasaran:</p>
<ol>
<li>Pemerintah
Sebagai organisasi tingkat tertinggi di sebuah negara, pemerintah dapat membuat kebijakan-kebijakan dan perubahan yang baik, guna meningkatkan kualitas hidup rakyatnya. Salah satu caranya ialah membuat/memperbaiki sistem di negaranya untuk mendorong kemajuan sumber daya manusianya.</li>
<li>Individu
Pada tingkat individu, diharapkan hasil proyek ini dapat memberikan insight terhadap faktor-faktor penting yang dapat meningkatkan kualitas hidupnya, melalui pekerjaan dan pekerjaan dengan pendapatan per tahun yang tinggi.</li>
</ol>
<p><strong>Problem Statements</strong></p>
<ol>
<li>Dari berbagai fitur, apa yang paling berpengaruh terhadap income (pendapatan)?</li>
<li>Dengan karakteristik tertentu, apakah income dapat diprediksi?</li>
</ol>
<p>*income merujuk pada pendapatan per tahun.</p>
<p><strong>Predictive Modelling Goals</strong></p>
<ol>
<li>Mengetahui fitur-fitur yang memiliki kaitan yang tinggi terhadap income.</li>
<li>Dapat memprediksi income dengan akurasi di atas 90%.</li>
</ol>
<p><strong>Solution Statements (Metodologi)</strong></p>
<ol>
<li>
<p>Target feature pada dataset ini merupakan variable boolean antara &gt;50k dan &lt;= 50k, sehingga kasus ini merupakan kasus prediksi Klasifikasi.</p>
</li>
<li>
<p>Melakukan Exploratory Data Analysis untuk mendapatkan informasi berguna dalam data dan mengetahui dinamika fitur-fitur.</p>
</li>
<li>
<p>Melakukan uji perbedaan teknik-teknik <em>missing values handling</em>, dan dampaknya terhadap akurasi model machine learning.</p>
</li>
<li>
<p>Membuat model machine learning yang dapat memprediksi income dengan akurasi di atas 90%</p>
</li>
<li>
<p>Menggunakan metrik evaluasi Accuracy, Precision, Recall, F1-Score, dan Confusion Matrix untuk mengevaluasi performa model.</p>
</li>
</ol>
<h2 id="data-understanding"><em><strong>Data Understanding</strong></em></h2>
<p>Dataset: https://archive.ics.uci.edu/dataset/2/adult
<em><strong>Dataset Overview</strong></em></p>
<p>Deskripsi variabel pada dataset adalah sebagai berikut:</p>
<ol>
<li>age (int64) : umur sampel</li>
<li>workclass (object) : status pekerjaan sampel</li>
<li>fnlwgt (int64) : ID sampel</li>
<li>education (object) : pendidikan terakhir sampel</li>
<li>educational-num (int64) : pendidikan terakhir sampel dalam angka</li>
<li>marital-status (object) : status pernikahan sampel</li>
<li>occupation (object) : pekerjaan/industri pekerjaan sampel</li>
<li>relationship (object) : status hubungan sampel</li>
<li>race (object) : ras sampel</li>
<li>gender (object) : jenis kelamin sampel</li>
<li>capital-gain (int64) : pemasukan kapital</li>
<li>capital-loss (int64) : kerugian kapital</li>
<li>hours-per-week (int64) : jam kerja sampel dalam seminggu</li>
<li>native-country (object) : negara asal/buyut sampel</li>
<li>income (object) : pendapatan sampel</li>
</ol>
<p>Deskripsi rinci dari dataset ini adalah sebagai berikut</p>
<ol>
<li>Terdapat 48.842 sampel data</li>
<li>Terdapat dua tipe data yang berbeda, antara lain 'object' (kategorikal), dan 'int64' (numerik).</li>
<li>Dari 15 kolom variabel, terdapat 9 kolom kategorikal dan 6 kolom numerik.</li>
<li>Pembagiannya adalah 14 kolom fitur dan 1 kolom target.</li>
<li>Pada dataset ini, missing values, dtiandai dengan tanda '?', dan berjumlah sekitar 7% dari seluruh dataset.</li>
</ol>
<p><strong>Deskripsi dari distribusi data dapat dicermati pada bagian <em>Univariate Analysis</em> dan <em>Multivariate Analysis</em> di bawah ini</strong>.</p>
<h2 id="univariate-analysis"><em><strong>Univariate Analysis</strong></em></h2>
<p><img src="Sub1_PA/univariateWorkclass.png?raw=true" alt="Univariate Visualization" title="Univariate result"></p>
<p>Gambar 1.0: Univariate analysis workclass vs income</p>
<p>Keterangan dari distribusi workclass dapat dicermati dari tabel berikut.</p>
<p>Tabel 1.0: Distribusi Workclass</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>workclass</strong></th>
<th style="text-align:center"><strong>sampe count</strong></th>
<th style="text-align:center"><strong>percentage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Private</td>
<td style="text-align:center">33906</td>
<td style="text-align:center">69.4</td>
</tr>
<tr>
<td style="text-align:center">Self-emp-not-inc</td>
<td style="text-align:center">3862</td>
<td style="text-align:center">7.9</td>
</tr>
<tr>
<td style="text-align:center">Local-gov</td>
<td style="text-align:center">3136</td>
<td style="text-align:center">6.4</td>
</tr>
<tr>
<td style="text-align:center">?</td>
<td style="text-align:center">2799</td>
<td style="text-align:center">5.7</td>
</tr>
<tr>
<td style="text-align:center">State-gov</td>
<td style="text-align:center">1981</td>
<td style="text-align:center">4.1</td>
</tr>
<tr>
<td style="text-align:center">Self-emp-inc</td>
<td style="text-align:center">1695</td>
<td style="text-align:center">3.5</td>
</tr>
<tr>
<td style="text-align:center">Federal-gov</td>
<td style="text-align:center">1432</td>
<td style="text-align:center">2.9</td>
</tr>
<tr>
<td style="text-align:center">Without-pay</td>
<td style="text-align:center">21</td>
<td style="text-align:center">0.0</td>
</tr>
<tr>
<td style="text-align:center">Never-worked</td>
<td style="text-align:center">10</td>
<td style="text-align:center">0.0</td>
</tr>
</tbody>
</table>
<p>Mencermati tabel 1.0, dapat dilihat bahwa kategori modus dari workclass adalah Private, dengan selisih lebih dari 60%.</p>
<hr>
<p><img src="Sub1_PA/univariateEduclass.png?raw=true" alt="Univariate Visualization" title="Univariate result"></p>
<p>Gambar 1.1: Univariate analysis educationClass vs income</p>
<p>Keterangan dari distribusi educationClass dicermati dari tabel berikut.</p>
<p>Tabel 1.1: Distribusi educationClass</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>educationClass</strong></th>
<th style="text-align:center"><strong>sampe count</strong></th>
<th style="text-align:center"><strong>percentage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">HS-grad</td>
<td style="text-align:center">15784</td>
<td style="text-align:center">32.3</td>
</tr>
<tr>
<td style="text-align:center">Some-college</td>
<td style="text-align:center">10878</td>
<td style="text-align:center">22.3</td>
</tr>
<tr>
<td style="text-align:center">Bachelors</td>
<td style="text-align:center">8025</td>
<td style="text-align:center">16.4</td>
</tr>
<tr>
<td style="text-align:center">Masters</td>
<td style="text-align:center">2657</td>
<td style="text-align:center">5.4</td>
</tr>
<tr>
<td style="text-align:center">Assoc-voc</td>
<td style="text-align:center">2061</td>
<td style="text-align:center">4.2</td>
</tr>
<tr>
<td style="text-align:center">11th</td>
<td style="text-align:center">1812</td>
<td style="text-align:center">3.7</td>
</tr>
<tr>
<td style="text-align:center">Assoc-acdm</td>
<td style="text-align:center">1601</td>
<td style="text-align:center">3.3</td>
</tr>
<tr>
<td style="text-align:center">10th</td>
<td style="text-align:center">1389</td>
<td style="text-align:center">2.8</td>
</tr>
<tr>
<td style="text-align:center">7th-8th</td>
<td style="text-align:center">955</td>
<td style="text-align:center">2.0</td>
</tr>
<tr>
<td style="text-align:center">Prof-school</td>
<td style="text-align:center">834</td>
<td style="text-align:center">1.7</td>
</tr>
<tr>
<td style="text-align:center">9th</td>
<td style="text-align:center">756</td>
<td style="text-align:center">1.5</td>
</tr>
<tr>
<td style="text-align:center">12th</td>
<td style="text-align:center">657</td>
<td style="text-align:center">1.3</td>
</tr>
<tr>
<td style="text-align:center">Doctorate</td>
<td style="text-align:center">594</td>
<td style="text-align:center">1.2</td>
</tr>
<tr>
<td style="text-align:center">5th-6th</td>
<td style="text-align:center">509</td>
<td style="text-align:center">1.0</td>
</tr>
<tr>
<td style="text-align:center">1st-4th</td>
<td style="text-align:center">247</td>
<td style="text-align:center">0.5</td>
</tr>
<tr>
<td style="text-align:center">Preschool</td>
<td style="text-align:center">83</td>
<td style="text-align:center">0.2</td>
</tr>
</tbody>
</table>
<p>Pada tabel 1.1, dapat dilihat bahwa kategori modus jatuh pada HS-grad (lulusan SMA), namun distribusi terlihat lebih seimbang daripada kategori Workclass, dengan tren <em>Right-skewed (positive) distribution</em>.</p>
<hr>
<p><img src="Sub1_PA/univariateStatus.png?raw=true" alt="Univariate Visualization" title="Univariate result"></p>
<p>Gambar 1.2: Univariate analysis status vs income</p>
<p>Keterangan dari distribusi status dapat dicermati dari tabel berikut.</p>
<p>Tabel 1.2: Distribusi status</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>status</strong></th>
<th style="text-align:center"><strong>sampe count</strong></th>
<th style="text-align:center"><strong>percentage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Married-civ-spouse</td>
<td style="text-align:center">22379</td>
<td style="text-align:center">45.8</td>
</tr>
<tr>
<td style="text-align:center">Never-married</td>
<td style="text-align:center">16117</td>
<td style="text-align:center">33.0</td>
</tr>
<tr>
<td style="text-align:center">Divorced</td>
<td style="text-align:center">6633</td>
<td style="text-align:center">13.6</td>
</tr>
<tr>
<td style="text-align:center">Separated</td>
<td style="text-align:center">1530</td>
<td style="text-align:center">3.1</td>
</tr>
<tr>
<td style="text-align:center">Widowed</td>
<td style="text-align:center">1518</td>
<td style="text-align:center">3.1</td>
</tr>
<tr>
<td style="text-align:center">Married-spouse-absent</td>
<td style="text-align:center">628</td>
<td style="text-align:center">1.3</td>
</tr>
<tr>
<td style="text-align:center">Married-AF-spouse</td>
<td style="text-align:center">37</td>
<td style="text-align:center">0.1</td>
</tr>
</tbody>
</table>
<p>Berdasarkan tabel 1.2, mirip dengan educationClass, distribusi terlihat lebih seimbang dan tidak tinggi pada satu kategori, dengan tren <em>Right-skewed distribution</em></p>
<hr>
<p><img src="Sub1_PA/univariateOcc.png?raw=true" alt="Univariate Visualization" title="Univariate result"></p>
<p>Gambar 1.3: Univariate analysis occupation vs income</p>
<p>Keterangan dari distribusi occupation dapat dicermati dari tabel berikut.</p>
<p>Tabel 1.3: Distribusi occupation</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>occupation</strong></th>
<th style="text-align:center"><strong>sampe count</strong></th>
<th style="text-align:center"><strong>percentage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Prof-specialty</td>
<td style="text-align:center">6172</td>
<td style="text-align:center">12.6</td>
</tr>
<tr>
<td style="text-align:center">Craft-repair</td>
<td style="text-align:center">6112</td>
<td style="text-align:center">12.5</td>
</tr>
<tr>
<td style="text-align:center">Exec-managerial</td>
<td style="text-align:center">6086</td>
<td style="text-align:center">12.5</td>
</tr>
<tr>
<td style="text-align:center">Adm-clerical</td>
<td style="text-align:center">5611</td>
<td style="text-align:center">11.5</td>
</tr>
<tr>
<td style="text-align:center">Sales</td>
<td style="text-align:center">5504</td>
<td style="text-align:center">11.3</td>
</tr>
<tr>
<td style="text-align:center">Other-service</td>
<td style="text-align:center">4923</td>
<td style="text-align:center">10.1</td>
</tr>
<tr>
<td style="text-align:center">Machine-op-inspct</td>
<td style="text-align:center">3022</td>
<td style="text-align:center">6.2</td>
</tr>
<tr>
<td style="text-align:center">?</td>
<td style="text-align:center">2809</td>
<td style="text-align:center">5.8</td>
</tr>
<tr>
<td style="text-align:center">Transport-moving</td>
<td style="text-align:center">2355</td>
<td style="text-align:center">4.8</td>
</tr>
<tr>
<td style="text-align:center">Handlers-cleaners</td>
<td style="text-align:center">2072</td>
<td style="text-align:center">4.2</td>
</tr>
<tr>
<td style="text-align:center">Farming-fishing</td>
<td style="text-align:center">1490</td>
<td style="text-align:center">3.1</td>
</tr>
<tr>
<td style="text-align:center">Tech-support</td>
<td style="text-align:center">1446</td>
<td style="text-align:center">3.0</td>
</tr>
<tr>
<td style="text-align:center">Protective-serv</td>
<td style="text-align:center">983</td>
<td style="text-align:center">2.0</td>
</tr>
<tr>
<td style="text-align:center">Priv-house-serv</td>
<td style="text-align:center">242</td>
<td style="text-align:center">0.5</td>
</tr>
<tr>
<td style="text-align:center">Armed-Forces</td>
<td style="text-align:center">15</td>
<td style="text-align:center">0.0</td>
</tr>
</tbody>
</table>
<p>Pada tabel 1.3, distribusi semakin normal, dengan tren ringan <em>Right-skewed distribution</em>.</p>
<hr>
<p><img src="Sub1_PA/univariateRel.png?raw=true" alt="Univariate Visualization" title="Univariate result"></p>
<p>Gambar 1.4: Univariate analysis relationship vs income</p>
<p>Keterangan dari distribusi relationship dapat dicermati dari tabel berikut.</p>
<p>Tabel 1.4: Distribusi relationship</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>relationship</strong></th>
<th style="text-align:center"><strong>sample count</strong></th>
<th style="text-align:center"><strong>percentage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Husband</td>
<td style="text-align:center">19716</td>
<td style="text-align:center">40.4</td>
</tr>
<tr>
<td style="text-align:center">Not-in-family</td>
<td style="text-align:center">12583</td>
<td style="text-align:center">25.8</td>
</tr>
<tr>
<td style="text-align:center">Own-child</td>
<td style="text-align:center">7581</td>
<td style="text-align:center">15.5</td>
</tr>
<tr>
<td style="text-align:center">Unmarried</td>
<td style="text-align:center">5125</td>
<td style="text-align:center">10.5</td>
</tr>
<tr>
<td style="text-align:center">Wife</td>
<td style="text-align:center">2331</td>
<td style="text-align:center">4.8</td>
</tr>
<tr>
<td style="text-align:center">Other-relative</td>
<td style="text-align:center">1506</td>
<td style="text-align:center">3.1</td>
</tr>
</tbody>
</table>
<p>Kemudian pada tabel 1.4, dapat dicermati bahwa tren dataset cukup seimbang, dengan nilai tertinggi Husband. Hasil ini dapat menyiratkan bahwa distribusi Gender akan lebih tinggi pada pria daripada wanita. Hal ini akan dikonfirmasikan pada Univariate Analysis pada fitur Gender.</p>
<hr>
<p><img src="Sub1_PA/univariateRace.png?raw=true" alt="Univariate Visualization" title="Univariate result"></p>
<p>Gambar 1.5: Univariate analysis race vs income</p>
<p>Keterangan dari distribusi race dapat dicermati dari tabel berikut.</p>
<p>Tabel 1.5: Distribusi race</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>race</strong></th>
<th style="text-align:center"><strong>sample count</strong></th>
<th style="text-align:center"><strong>percentage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">White</td>
<td style="text-align:center">41762</td>
<td style="text-align:center">85.5</td>
</tr>
<tr>
<td style="text-align:center">Black</td>
<td style="text-align:center">4685</td>
<td style="text-align:center">9.6</td>
</tr>
<tr>
<td style="text-align:center">Asian-Pac-Islander</td>
<td style="text-align:center">1519</td>
<td style="text-align:center">3.1</td>
</tr>
<tr>
<td style="text-align:center">Amer-Indian-Eskimo</td>
<td style="text-align:center">470</td>
<td style="text-align:center">1.0</td>
</tr>
<tr>
<td style="text-align:center">Other</td>
<td style="text-align:center">406</td>
<td style="text-align:center">0.8</td>
</tr>
</tbody>
</table>
<p>Dari tabel 1.5, dapat terlihat bahwa modus yang sangat jelas jatuh pada kategori White, dengan perbedaan hingga 75,9%.</p>
<hr>
<p><img src="Sub1_PA/univariateGender.png?raw=true" alt="Univariate Visualization" title="Univariate result"></p>
<p>Gambar 1.6: Univariate analysis gender vs income</p>
<p>Keterangan dari distribusi gender dapat dicermati dari tabel berikut.</p>
<p>Tabel 1.6: Distribusi gender</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>gender</strong></th>
<th style="text-align:center"><strong>sample count</strong></th>
<th style="text-align:center"><strong>percentage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Male</td>
<td style="text-align:center">32650</td>
<td style="text-align:center">66.8</td>
</tr>
<tr>
<td style="text-align:center">Female</td>
<td style="text-align:center">16192</td>
<td style="text-align:center">33.2</td>
</tr>
</tbody>
</table>
<p>Hasil dari tabel 1.6 membenarkan teori dari tabel 1.4, dimana distribusi Male (pria) lebih besar dari Female (wanita).</p>
<hr>
<p><img src="Sub1_PA/univariateNative.png?raw=true" alt="Univariate Visualization" title="Univariate result"></p>
<p>Gambar 1.7: Univariate analysis native vs income</p>
<p>Keterangan dari distribusi native dapat dicermati dari tabel berikut.</p>
<p>*nilai yang ditampilkan hanya 15 teratas, karena unique value yang terlalu banyak.</p>
<p>Tabel 1.7: Distribusi native</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>native</strong></th>
<th style="text-align:center"><strong>sample count</strong></th>
<th style="text-align:center"><strong>percentage</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">USA</td>
<td style="text-align:center">43832</td>
<td style="text-align:center">89.7</td>
</tr>
<tr>
<td style="text-align:center">Mexico</td>
<td style="text-align:center">951</td>
<td style="text-align:center">1.9</td>
</tr>
<tr>
<td style="text-align:center">?</td>
<td style="text-align:center">857</td>
<td style="text-align:center">1.8</td>
</tr>
<tr>
<td style="text-align:center">Philippines</td>
<td style="text-align:center">295</td>
<td style="text-align:center">0.6</td>
</tr>
<tr>
<td style="text-align:center">Germany</td>
<td style="text-align:center">206</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">PuertoRic</td>
<td style="text-align:center">184</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">Canada</td>
<td style="text-align:center">182</td>
<td style="text-align:center">0.4</td>
</tr>
<tr>
<td style="text-align:center">El-Salvador</td>
<td style="text-align:center">155</td>
<td style="text-align:center">0.3</td>
</tr>
<tr>
<td style="text-align:center">India</td>
<td style="text-align:center">151</td>
<td style="text-align:center">0.3</td>
</tr>
<tr>
<td style="text-align:center">Cuba</td>
<td style="text-align:center">138</td>
<td style="text-align:center">0.3</td>
</tr>
<tr>
<td style="text-align:center">England</td>
<td style="text-align:center">127</td>
<td style="text-align:center">0.3</td>
</tr>
<tr>
<td style="text-align:center">China</td>
<td style="text-align:center">122</td>
<td style="text-align:center">0.2</td>
</tr>
<tr>
<td style="text-align:center">South</td>
<td style="text-align:center">115</td>
<td style="text-align:center">0.2</td>
</tr>
<tr>
<td style="text-align:center">Jamaica</td>
<td style="text-align:center">106</td>
<td style="text-align:center">0.2</td>
</tr>
<tr>
<td style="text-align:center">Italy</td>
<td style="text-align:center">105</td>
<td style="text-align:center">0.2</td>
</tr>
</tbody>
</table>
<p>Pada tabel 1.7 dapat dicermati bahwa modus jatuh pada kategori USA, dengan perbedaan persentase hingga 87,8% dengan kategori berikutnya.</p>
<hr>
<p><img src="Sub1_PA/univariateNum.png?raw=true" alt="Univariate Visualization" title="Univariate result">
Gambar 1.8: Univariate analysis distribusi numerical features</p>
<p>Pada gambar 1.8, dapat dilihat bahwa distribusi numerikal mayoritas memiliki nilai modus dengan perbedaan yang sangat tinggi dibandingkan dengan nilai-nilai lainnya.</p>
<hr>
<p><em><strong>Analisis dan interpretasi hasil Univariate Analysis:</strong></em></p>
<ol>
<li><em>Missing values</em> ditandai dengan '?'.</li>
<li>Mayoritas dari distribusi variabel sangat berat kepada modus, dengan perbedaan persentase yang sangat besar. Hal ini menyiratkan bahwa dataset ini dapat diatasi <em>missing values</em>-nya dengan metode statistika, seperti <em>Mode Imputation</em>.</li>
</ol>
<h2 id="multivariate-analysis"><em><strong>Multivariate Analysis</strong></em></h2>
<p><img src="Sub1_PA/workclass1.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.0: Multivariate analysis workclass vs income</p>
<p>Pada workclass, dapat dilihat bahwa semua workclass memiliki income, kecuali tentunya never-worked. Rata-rata yang bekerja pada pemerintah (-gov) memiliki income yang lebih tinggi dibandingkan dengan workclass lain, namun self-emp-inc memiliki pendapatan paling tinggi.</p>
<hr>
<p><img src="Sub1_PA/educationclass2.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.1: Multivariate analysis educationClass vs income</p>
<p>Pada educationClass, dapat dilihat dengan jelas bahwa terdapat kesenjangan yang sangat tinggi mulai dari tingkat pendidikan S1 (bachelors), dan lebih tinggi lagi semakin tinggi gelar yang dimiliki (masters, doctorate, prof-school).</p>
<hr>
<p><img src="Sub1_PA/multivariateVis.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.2: Multivariate analysis status vs income</p>
<p>Pada status pernikahan, dapat dilihat pula tren bahwa data dengan status pernikahan stabil memiliki pendapatan yang lebih tinggi dibandingkan mereka yang tidak menikah, bercerai, berpisah, atau cerai mati.</p>
<hr>
<p><img src="Sub1_PA/occ4.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.3: Multivariate analysis occupation vs income</p>
<p>Pada occupation, tiga sektor kerja paling tinggi pendapatannya adalah Specialty (spesialis), manajerial (eksekutif), dan protective service (jasa keamanan). Selain itu, tidak terdapat tren yang secara jelas dapat di diamati.</p>
<hr>
<p><img src="Sub1_PA/relationship5.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.4: Multivariate analysis relationship vs income</p>
<p>Pada relationship, dapat dilihat bahwa menguatkan hasil pengamatan status pernikahan, yang berstatus sebagai husband (suami) dan istri (istri) memiliki pendapatan yang tertinggi dari data-data lain.</p>
<hr>
<p><img src="Sub1_PA/race6.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.5: Multivariate analysis race vs income</p>
<p>Pada race, dua ras dengan pendapatan tertinggi adalah White (putih) dan Asian-Pac (asia-pasifik)</p>
<hr>
<p><img src="Sub1_PA/gender7.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.6: Multivariate analysis gender vs income</p>
<p>Pada gender, dapat dilihat bahwa pendapatan laki-laki lebih tinggi dari pendapatan perempuan.</p>
<hr>
<p><img src="Sub1_PA/native8.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.7: Multivariate analysis native vs income</p>
<p>pada native (negara asal/buyut), dapat dilihat bahwa tren pendapatan kurang dapat diamati, artinya negara asal/buyut bukan faktor yang kuat yang dapat memengaruhi pendapatan.</p>
<hr>
<p><img src="Sub1_PA/multivariateVisNum.png?raw=true" alt="Multivariate Visualization" title="Multivariate result">
Gambar 2.8: Multivariate analysis numerical-values vs income</p>
<p>Pada visualisasi fitur numerik terhadap income, dapat dilihat bahwa kurang terlihat pola dalam pengaruh fitur numerik. Hal ini menyiratkan bahwa nilai korelasi antar fitur numerik dan income tidak terlalu signifikan.</p>
<hr>
<p><em><strong>Analisis dan interpretasi hasil Multivariate Analysis:</strong></em></p>
<ol>
<li>
<p>Hasil dari Multivariate Analysis menunjukkan bahwa tren yang paling jelas, ada pada educationClass, status, dan relationship.</p>
</li>
<li>
<p>Pada gambar 2.1 yang memvisualisasikan analisis educationClass terhadap income, dapat dilihat bahwa terjadi lompatan yang tinggi dimulai dari pendidikan S1, dan income semaking tinggi seiring dengan semakin tingginya derajat pendidikan.</p>
</li>
<li>
<p>Pada gambar 2.2 dan 2.4, status dan relationship, terlihat bahwa sampel yang berada dalam hubungan pernikahan yang sehat akan memiliki income yang lebih tinggi dibandingkan status yang lain, dan ini dikuatkan pada analisis terhadap relationship.</p>
</li>
</ol>
<p><img src="sub1_pa/corrMat.png?raw=true" alt="Correlation matrix" title="Correlation Matrix">
Gambar 3.0: Correlation matrix variabel-variabel</p>
<p>Hasil dari correlation matrix di atas menunjukkan bahwa fitur yang kuat seperti educationClass tidak terlalu jauh nilai korelasinya dengan nilai rendah, seperti capital-gain. Maka dari itu, tidak akan dilakukan drop kolom lagi.</p>
<h2 id="data-preparation"><em><strong>Data Preparation</strong></em></h2>
<p>Secara berurutan, proses Data Preparation yang akan dilakukan adalah</p>
<ol>
<li>
<p>Features Encoding</p>
<ul>
<li>Features Encoding adalah pengubahan fitur-fitur kategorikal menjadi representasi numerik. Alasan diperlukan Features Encoding adalah sebagai berikut:
<ul>
<li>Meskipun terdapat model yang dapat mengolah data kategorikal, mayoritas dari model yang ada tetap membutuhkan input dalam bentuk numerik.</li>
<li>Lanjutan dari poin pertama, terkadang terdapat hubungan tingkatan (ordinalitas) yang ada dalam dataset. Merepresentasikan data kategorikal menjadi numerik dapat membantu mengawetkan hubungan ini, sehingga dimengerti secara implisit oleh model.</li>
<li>Model machine learning adalah model berbasis matematika dan statistika, sehingga pengolahan dalam bentuk numerik akan mempercepat proses training dan testing.</li>
</ul>
</li>
<li>Teknik-teknik Features Encoding yang akan dilakukan pada proyek ini adalah:
<ol>
<li>Ordinal Encoding</li>
<li>One Hot Encoding</li>
</ol>
</li>
</ul>
</li>
<li>
<p>Train Test Split</p>
<ul>
<li>Train Test Split adalah prosedur standar dalam melakukan Machine Learning, dimana dataset akan dibagi menjadi data latih (train) dan test. Hal ini dilakukan untuk memastikan bahwa model yang telah dilatih dapat menyelesaikan masalah pada data asli yang belum pernah ditemuinya.</li>
</ul>
</li>
<li>
<p>KNN Imputation</p>
<ul>
<li>KNN Imputation adalah salah satu proses yang akan diuji di proyek ini, dimana missing values akan diisi dengan melihat kemiripan fitur-fitur lain pada baris yang sama, dan menginput nilai baru. KNN Imputation dapat menyebabkan Data Leakage, sehingga perlu dianggap sebagai proses transformasi, dari pada pre-processing. Dikarenakan itulah KNN Imputation dilakukan setelah proses Train Test Split.</li>
</ul>
</li>
<li>
<p>Standardization</p>
<ul>
<li>Standardization juga merupakan prosedur standar dalam melakukan Machine Learning. Menurut Google Machine Learning Developers <a href="https://developers.google.com/machine-learning/data-prep/transform/normalization#:~:text=The%20goal%20of%20normalization%20is,training%20stability%20of%20the%20model">[5]</a>, standarisasi adalah metode untuk mengubah nilai-nilai numerik pada data ke skala yang sangat mirip, untuk meningkatkan performa dan stabilitas pada model saat train dan test.</li>
</ul>
</li>
</ol>
<p>Setelah penjelasan terkait proyek di atas, berikut adalah alur yang dilewati pada tahap Data Preparation ini.</p>
<ol>
<li>
<p>Features Encoding.
Input pada tahap ini adalah tiga DataFrame, dengan keterangan:</p>
<ul>
<li>Satu DataFrame asli, masih terdapat missing values, dan</li>
<li>Dua DataFrame yang sama persis dimana sudah dilakukan proses Drop.
Pada DataFrame asli dan satu DataFrame Drop, dilakukan Ordinal Encoding, dan One Hot Encoding pada satu DataFrame yang tersisa. Sehingga, output dari tahap ini adalah tiga DataFrame,
<ul>
<li>Dropped One Hot Encoded,</li>
<li>Dropped Ordinal Encoded, dan</li>
<li>Data Asli Ordinal Encoded.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Train Test Split
Train Test Split dilakukan ke setiap DataFrame, memisahkan mereka menjadi x dan y.</p>
</li>
<li>
<p>KNN Imputation
KNN Imputation hanya dilakukan ke DataFrame Data Asli Ordinal Encoded. KNN Imputation dilakukan setelah Train Test Split untuk mencegah terjadinya Data Leakage.</p>
</li>
<li>
<p>Standarisasi
Pada tahap ini standarisasi dilakukan ke semua x_train dan x_test pada semua DataFrame, menyiapkan data tersebut untuk train dan test.</p>
</li>
</ol>
<h2 id="model-development"><em><strong>Model Development</strong></em></h2>
<p>Proses Model Development yang akan Penulis lakukan dapat dibagi menjadi beberapa tahap:</p>
<ol>
<li>
<p>DataFrame initialization: menyiapkan dataframe untuk menyimpan hasil training model.</p>
</li>
<li>
<p>Pipeline Prep: menyiapkan pipeline untuk mempermudah proses training.</p>
</li>
<li>
<p>Model Training: melatih model.</p>
</li>
<li>
<p>Visualization</p>
</li>
</ol>
<p>Pembuatan model dan training dilakukan di saat yang hampir bersamaan, dengan menggunakan metode Pipeline.</p>
<p><em><strong>Model Explanation, pros and cons</strong></em></p>
<hr>
<p>Pada proyek ini, Penulis menggunakan empat algoritma, antara lain Random Forest, K-Nearest Neighbor, XGBoost, dan AdaBoost.</p>
<ol>
<li><strong>Random Forest</strong></li>
</ol>
<ul>
<li>
<p>Random Forest adalah Supervised Learning Based Algorithm, dan merupakan implementasi dari ensemble learning. Model Ensemble sendiri adalah sekelompok model yang bekerja sama untuk meningkatkan performa prediksi. Random Forest merupakan ensemble (kumpulan) dari banyak model <em>decision tree</em>, yang digabungkan dengan teknik <em>Bagging</em>, dimana dari masing-masing decision tree akan menghasilkan sebuah prediksi, <em>Bagging</em> pada kasus klasifikasi akan mengambil prediksi terbanyak pada seluruh pohon sebagai prediksi akhir. Hal ini sangat cocok untuk proyek ini karena sifat ini membuat Random Forest tidak rentan terhadap bias.</p>
</li>
<li>
<p>Dalam proyek ini, parameter untuk algoritma Random Forest yang digunakan adalah:</p>
<ul>
<li>n_estimator = 50
n_estimator merupakan parameter yang menentukan jumlah pohon (decision tree) yang akan digunakan pada algortima Random Forest. Penulis menggunakan 50 untuk percobaan dan akurasi yang didapatkan pun sudah cukup baik. Karena akurasi dari algoritma Random Forest bukan tujuan prioritas dari proyek, 50 pohon dirasa cukup.</li>
<li>max_depth = 16
max_depth merupakan parameter yang menentukan kedalaman pohon, lebih tepatnya seberapa banyak pohon dapat membelah untuk melakukan komputasi/pengamatan. Sama seperti parameter pertama, akurasi sudah baik, sehingga max_depth = 16 dirasa sudah cukup.</li>
<li>random_state = 55
random_state merupakan parameter untuk mengatur generator random untuk memastikan setiap jalannya proses training/testing konsisten.</li>
<li>n_jobs = -1
n_jobs merupakan parameter yang digunakan untuk mengatur berapa jumlah pekerjaan yang berjalan secara paralel. n_jobs = -1 berarti semua proses berjalan secara paralel.</li>
</ul>
</li>
<li>
<p>Dari penjelasan singkat terkait algoritma Random Forest di atas, beberapa kelebihan dan kekurangannya adalah sebagai berikut:</p>
<ul>
<li>Kelebihan:</li>
</ul>
<ol>
<li>Dapat digunakan untuk klasifikasi maupun regresi.</li>
<li>Dapat digunakan pada data kategorikal maupun numerikal. Dapat bekerja tanpa scaling dan transformation sekalipun.</li>
<li>Dapat melakukan feature selection secara implisit, dan tahan terhadap outliers.</li>
<li>Dapat bekerja pada problem linier maupun non-linier, tahan pada bias, dan akurat.</li>
</ol>
<ul>
<li>Kekurangan:</li>
</ul>
<ol>
<li>Mahal secara komputasi, terutama pada dataset besar.</li>
<li>Tidak fleksibel, tidak terlalu banyak yang dapat diatur sendiri. (Kurang efektif untuk hyperparameter tuning)</li>
</ol>
</li>
<li>
<p><strong>K-Nearest Neighbor</strong>
KNN bekerja dengan menggunakan kesamaan fitur untuk memprediksi nilai dari setiap data yang baru. Secara matetmatis, algortima KNN menghitung jarak (Euclidean) antara setiap poin data dan memilih klasifikasi berdasarkan mayoritas tetangga terdekat. Jika dilihat dari kompleksitas alrgortima, KNN dapat dikategorikan dalam algoritma yang lebih sederhana. KNN menjadi algoritma yang sering digunakan, termasuk pada proyek ini, karena sifatnya yang sederhana, dan tidak memiliki asumsi. Namun perlu diperhatikan pada curse of Dimensionality, yaitu tidak terlalu efektif pada dataset dengan fitur yang sangat banyak.</p>
</li>
<li>
<p>Dalam proyek ini, parameter yang digunakan untuk algoritma KNN adalah:</p>
<ul>
<li>n_neighbors = 10
n_neighbors adalah parameter untuk mengatur berapa jumlah tetangga yang akan dipertimbangkan untuk proses penghitungan jarak Euclidean. Penulis memilih 10 tetangga, dikarenakan terdapat beberapa fitur yang sangat lemah korelasinya, sehingga diharapkan bahwa fitur lemah tersebut tidak diprioritaskan dalam proses training.</li>
</ul>
</li>
<li>
<p>Dari penjelasan singkat terkait algoritma KNN di atas, beberapa kelebihan dan kekurangannya adalah sebagai berikut:</p>
<ul>
<li>Kelebihan:</li>
</ul>
<ol>
<li>Sederhana, intuitif, dan mudah digunakan.</li>
<li>Sangat efektif pada problem multi-class.</li>
<li>Mudah dituning karena jarak dapat diatur antara Euclidean, Hamming, Manhattan, Minkowski, dst.</li>
<li>Tidak ada asumsi.</li>
</ol>
<ul>
<li>Kekurangan:</li>
</ul>
<ol>
<li>Curse of Dimensionality, lemah terhadap dataset dengan dimensi besar.</li>
<li>KNN bukan algoritma tercepat.</li>
<li>Scaling dan Transformasi wajib dilakukan.</li>
<li>Lemah terhadap outliers, missing values, dan imbalanced dataset.</li>
</ol>
</li>
<li>
<p><strong>XGBoost</strong>
eXtreme Gradient Boosting (XGBoost) adalah model berbasis ensemble learning dan boosting, dan merupakan turunan dari framework Gradient Boosting Decision Tree. Boosting sendiri merupakan proses yang membuat dan menggabungkan weak learner models secara iteratif hingga menghasilkan suatu strong learner model. Cara boosting bekerja adalah membangun model secara berurutan dengan fokus pada data yang salah sebelumnya. Dalam kasus XGBoost, atau Gradient Boosting pada umumnya, iterasi pembuatan dan penggabungan model dilakukan berdasarkan gradien error (gradient of error) atau yang sering disebut Gradient Loss Function. Secara fundametnal, perbedaan XGBoost dan AdaBoost adalah loss function yang digunakan.</p>
</li>
<li>
<p>Dari penjelasan singkat terkait algoritma XGBoost di atas, beberapa kelebihan dan kekurangannya adalah sebagai berikut:</p>
<ul>
<li>Kelebihan:</li>
</ul>
<ol>
<li>Cepat.</li>
<li>Sangat tahan terhadap outliers</li>
<li>Fleksibel, mampu beradaptasi pada variasi data yang tinggi.</li>
<li>Built-in regularisasi.</li>
<li>Akurasi tinggi.</li>
</ol>
<ul>
<li>Kekurangan:</li>
</ul>
<ol>
<li>Mahal secara komputasi.</li>
<li>Kompleks.</li>
<li>Mirip seperti Random Forest, XGBoost kurang bisa dilakukan hyperparameter tuning.</li>
</ol>
</li>
<li>
<p><strong>AdaBoost</strong>
Adaptive Boosting (AdaBoost) adalah Supervised Learning Based Algorithm yang mengimplementasikan ensemble learning dan boosting. Mirip dengan XGBoost, AdaBoost bekerja dengan membuat sejumlah weak learners berbasis decision tree, kemudian membuat model-model berikutnya dengan jawaban salah dari model sebelumnya. Berbeda dengan XGBoost, AdaBoost menggunakan Exponential Loss Function.</p>
</li>
<li>
<p>Dalam proyek ini, parameter untuk algoritma AdaBoost yang digunakan adalah:</p>
<ul>
<li>n_estimators = 50
Mirip dengan algoritma Random Forest, AdaBoost menerima parameter n_estimators untuk mengatur jumlah decision trees yang akan dihasilkan. Penulis menggunakan 50 sebagai percobaan, dan hasil yang didapatkan sudah cukup baik.</li>
<li>random_state = 123
random_state merupakan parameter untuk mengatur generator random untuk memastikan setiap jalannya proses training/testing konsisten.</li>
</ul>
</li>
<li>
<p>Dari penjelasan singkat terkait algoritma AdaBoost di atas, beberapa kelebihan dan kekurangannya adalah sebagai berikut:</p>
<ul>
<li>Kelebihan:</li>
</ul>
<ol>
<li>Potensi tinggi, meningkatkan performa model secara signifikan.</li>
<li>Murah secara komputasi.</li>
<li>Fleksibel, dapat digunakan di berbagai kasus.</li>
<li>Dapat digunakan bersama model lain.</li>
</ol>
<ul>
<li>Kekurangan:</li>
</ul>
<ol>
<li>Varians tinggi.</li>
<li>Kurang efektif dalam problem linier.</li>
<li>Lebih rentan terhadap outliers.</li>
</ol>
</li>
</ul>
<h2 id="evaluation"><em><strong>Evaluation</strong></em></h2>
<p>Dalam proyek ini, beberapa metrik evaluasi yang digunakan adalah sebagai berikut.
Sebelum memasuki penjelasan metrik lebih lanjut, perlu dipahami bahwa:</p>
<ul>
<li>TN = True Negative, data negatif yang diprediksi negatif (benar)</li>
<li>TP = True Positive, data positif yang diprediksi negatif (benar)</li>
<li>FN = False Negative, data negatif yang diprediksi positif (salah)</li>
<li>FP = False positive, data positif yang diprediksi negatif (salah)</li>
</ul>
<p>Pertama, Accuracy, yang dapat dihitung dengan rumus:
$$Accuracy = \frac{TN + TP}{TN + FP + TP + FN}$$</p>
<p>Accuracy merepresentasikan angka data yang benar di prediksi dibagi total jumlah data. Idealnya, akurasi memberikan ide seberapa baik model dapat memprediksi data, namun kekurangan dari metrik ini adalah kurang adilnya metrik jika dataset yang digunakan <em>unbalanced</em>.</p>
<p>Berikutnya adalah precision, yang dapat dihitung dengan rumus:</p>
<p>$$Precision = \frac{TP}{FP + TP}$$</p>
<p>Precision adalah rasio prediksi benar positif (TP) dari total prediksi positif. Semakin tinggi presisi, artinya semakin sedikit jumlah prediksi positif salah (FP).</p>
<p>Kemudian terdapat metrik Recall, seperti berikut.</p>
<p>$$Recall = \frac{TP}{TP + FN}$$</p>
<p>Recall menghitung nilai dari betulnya prediksi positif dari jumlah aktual positif. Semakin tinggi nilai recall berarti semakin sedikit False Negatives (FN).</p>
<p>Setelah menghitung metrik Accuracy, Precision, dan Recall, kita bisa mencari nilai F1 Score dengan rumus berikut.</p>
<p>$$F1 Score = 2* \frac{Precision * Recall}{Precision + Recall}$$</p>
<p>F1 Score adalah sebuah nilai harmonis yang menggunkan presisi dan Recall, artinya  nilai F1 Score yang tinggi memiliki Precision dan Recall yang tinggi.</p>
<p><strong>Model Training Results</strong></p>
<hr>
<ul>
<li>Visualization
<img src="sub1_pa/trainAcc.png?raw=true" alt="Train Acc" title="Train Accuracies Result">
Gambar 4.0: Hasil model training</li>
</ul>
<p>Table 2.0 Evaluasi training Accuracy, Precision, Recall, dan F1 Score data Dropped One Hot Encoded</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>KNN</td>
<td>0.864862</td>
<td>0.794974</td>
<td>0.612125</td>
<td>0.691669</td>
</tr>
<tr>
<td>RandomForest</td>
<td>0.877515</td>
<td>0.840669</td>
<td>0.623536</td>
<td>0.716003</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.88582</td>
<td>0.821095</td>
<td>0.689026</td>
<td>0.749285</td>
</tr>
<tr>
<td>AdaBoost</td>
<td>0.858154</td>
<td>0.770313</td>
<td>0.608653</td>
<td>0.680007</td>
</tr>
</tbody>
</table>
<p>Table 2.1 Evaluasi training Accuracy, Precision, Recall, dan F1 Score data Dropped Ordinal Encoded</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>KNN</td>
<td>0.860758</td>
<td>0.787511</td>
<td>0.599424</td>
<td>0.680714</td>
</tr>
<tr>
<td>RandomForest</td>
<td>0.89845</td>
<td>0.868751</td>
<td>0.69488</td>
<td>0.772148</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.887442</td>
<td>0.8215</td>
<td>0.696864</td>
<td>0.754067</td>
</tr>
<tr>
<td>AdaBoost</td>
<td>0.855402</td>
<td>0.767718</td>
<td>0.596547</td>
<td>0.671394</td>
</tr>
</tbody>
</table>
<p>Table 2.2 Evaluasi training Accuracy, Precision, Recall, dan F1 Score data KNN Imputed Ordinal Encoded</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>KNN</td>
<td>0.86414</td>
<td>0.787736</td>
<td>0.589262</td>
<td>0.674195</td>
</tr>
<tr>
<td>RandomForest</td>
<td>0.900721</td>
<td>0.859779</td>
<td>0.697597</td>
<td>0.770243</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.891053</td>
<td>0.82002</td>
<td>0.696071</td>
<td>0.752979</td>
</tr>
<tr>
<td>AdaBoost</td>
<td>0.859909</td>
<td>0.775178</td>
<td>0.581347</td>
<td>0.664414</td>
</tr>
</tbody>
</table>
<ol>
<li>
<p>Dari hasil training, dua model terbaik adalah Random Forest dan XGBoost. Hal ini dibuktikan bukan hanya dengan training accuracy yang sangat tinggi, melainkan F1 Score yang sangat tinggi juga. F1 Score yang tinggi merupakan indikasi bahwa model tersebut jarang melakukan kesalahan dalam prediksi nilai benar maupun salah. Penjelasan F1 Score akan dijelaskan lebih dalam pada tahap Evaluation.</p>
</li>
<li>
<p>Dapat diamati juga bahwa dari dataset drop One Hot Encoded dan Ordinal Encoded, terdapat perubahan yang cukup signifikan (peningkatan hingga dua persen) pada training accuracy. Hal ini menunjukkan pentingnya mengawetkan ordinalitas pada dataset (jika ada), yang dalam hal ini ada terutama pada fitur educationClass.</p>
</li>
<li>
<p>Pengujian prediksi terakhir akan menggunakan test data dari test_ord, dikarenakan dataset ini mengimplementasikan Ordinal Encoding dan memiliki tingkat integritas yang lebih tinggi dari dataset imputasi. Mengingat bahwa jumlah dataset ini melimpah dari awal, metode Drop tetap menjadi metode Data Handling terbaik karena pengawetan integritasnya.</p>
</li>
</ol>
<p><em><strong>Testing dan Prediksi Akhir</strong></em></p>
<hr>
<p>Melihat hasil dari hasil training diatas, terutama dari test_dum dan test_ord, dapat dilihat bahwa terjadi peningkatan akurasi dari dua model dengan akurasi tertinggi, yaitu Random Forest dan XGBoost. Hal ini membenarkan bahwa penting untuk menyimpan fitur ordinality (jika ada) yang dalam hal ini ada pada dataset, terutama educationClass.</p>
<p>Secara konsisten, model Random Forest dan XGBoost menghasilkan akurasi dan F1 Score yang tinggi, sebuah indikasi dari model yang unggul.</p>
<p>Pengujian prediksi terakhir akan menggunakan test data dari test_ord dikarenakan dataset ini mengimplementasikan metode Drop dan Ordinal Encoding sehingga dataset ini menjaga sifat ordinalitasnya dan memiliki tingkat integritas yang lebih tinggi dari dataset imputasi. Mengingat bahwa jumlah dataset ini melimpah dari awal, metode Drop tetap menjadi metode Data Handling terbaik karena pengawetan integritasnya.</p>
<p>Metode Testing akan sama dengan metode Training, menggunakan Pipeline untuk meningkatkan efisiensi Testing.</p>
<p><em><strong>Hasil Testing</strong></em></p>
<hr>
<p>Table 3.0 Evaluasi test Accuracy, Precision, Recall, dan F1 Score data Dropped One Hot Encoded</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>KNN</td>
<td>0.858943</td>
<td>0.770925</td>
<td>0.619469</td>
<td>0.686948</td>
</tr>
<tr>
<td>RandomForest</td>
<td>0.928366</td>
<td>0.930556</td>
<td>0.770796</td>
<td>0.843175</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.940084</td>
<td>0.909438</td>
<td>0.844248</td>
<td>0.875631</td>
</tr>
<tr>
<td>AdaBoost</td>
<td>0.86845</td>
<td>0.772126</td>
<td>0.671681</td>
<td>0.71841</td>
</tr>
</tbody>
</table>
<p>Table 3.1 Evaluasi test Accuracy,  Precision, Recall, dan F1 Score data Dropped Ordinal Encoded</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>KNN</td>
<td>0.853858</td>
<td>0.77686</td>
<td>0.582301</td>
<td>0.665655</td>
</tr>
<tr>
<td>RandomForest</td>
<td>0.954455</td>
<td>0.93097</td>
<td>0.883186</td>
<td>0.906449</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.941853</td>
<td>0.897342</td>
<td>0.866372</td>
<td>0.881585</td>
</tr>
<tr>
<td>AdaBoost</td>
<td>0.860491</td>
<td>0.759086</td>
<td>0.646903</td>
<td>0.698519</td>
</tr>
</tbody>
</table>
<p>Table 3.2 Evaluasi training Accuracy, Precision, Recall, dan F1 Score data KNN Imputed Ordinal Encoded</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>KNN</td>
<td>0.845855</td>
<td>0.762911</td>
<td>0.541216</td>
<td>0.63322</td>
</tr>
<tr>
<td>RandomForest</td>
<td>0.951894</td>
<td>0.917098</td>
<td>0.884263</td>
<td>0.900382</td>
</tr>
<tr>
<td>XGBoost</td>
<td>0.938588</td>
<td>0.895522</td>
<td>0.849292</td>
<td>0.871795</td>
</tr>
<tr>
<td>AdaBoost</td>
<td>0.854452</td>
<td>0.751025</td>
<td>0.610325</td>
<td>0.673404</td>
</tr>
</tbody>
</table>
<p>Ringkasan dan visualisasi dari metode-metode data handling, dan akurasinya dapat dilihat sebagai berikut.</p>
<p><img src="Sub1_pa/testAcc.png?raw=true" alt="Test Acc" title="Test Accuracies Result">
Gambar 5.0: Hasil testing</p>
<p>Hasil tes yang akan ditinjau, dan digunakan untuk prediksi akhir, adalah dari test data dengan Dropped Encoded. Hal ini dikarenakan metode Drop adalah metode terbaik dalam menjaga integritas data, dimana hasilnya dapat dilihat dari visualisasi Confusion Matrix berikut.</p>
<p><img src="sub1_pa/confusionMatrix.png?raw=true" alt="Confusion Matrix" title="Test Confusion Matrix">
Gambar 5.1: Confusion matrix testing</p>
<h2 id="hasil-dan-kesimpulan-proyek"><em><strong>Hasil dan Kesimpulan Proyek</strong></em></h2>
<p>Berdasarkan dari hasil Data Understanding, Data Preparation, Model Development, dan Evaluation, kita dapat menyimpulkan beberapa hal berikut.</p>
<ol>
<li>Menjawab Problem Statement 1: Fitur-fitur yang paling berpengaruh terhadap income pada problem ini adalah educationClass, status, dan relationship.</li>
<li>Menjawab Probelm Statement 2: <strong>Ya, income dapat diprediksi,</strong> dan berdasarkan nilai <em>Accuracy, Recall, Precision, dan F1 Score</em>, model Random Forest adalah model dengan performa terbaik, mencapai akurasi hingga 95,4%, presisi hingga 93%, recall 88,3%, dan F1Score 90,6%. Hal ini merupakan salah satu bentuk kelebihan dari Random Forest yaitu sifatnya yang melakukan <em>feature selection</em> secara implisit. Ditambah lagi sifat yang diwariskan dari <em>ensemble learning</em>, yaitu tahan pada bias dan overfitting.</li>
<li>Terdapat pengaruh yang dapat diamati dalam bentuk peningkatan performa model jika pengawetan ordinalitas (jika ada dalam dataset) dilakukan.</li>
<li><em>KNN Imputation</em> memiliki dampak yang kurang signifikan jika dibandingkan dengan poin nomor 3, namun performa tetap meningkat.</li>
</ol>
<p>Berdasarkan hasil-hasil penemuan di atas, dapat disimpulkan bahwa proyek ini berhasil dan berjalan sesuai dengan keinginan Penulis, yaitu menjawab Problem Statements dan mencapai Predictive Modelling Goals yang dirumuskan, serta berhasil mengobservasi perbedaan ordinalitas dan pengaruh KNN Imputation terhadap dataset.</p>
<h2 id="referensi"><em><strong>Referensi</strong></em></h2>
<p>[Dataset] Becker, Barry and Kohavi ,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20.</p>
<p>[1] Badan Pusat Statistika Indonesia. (2023). Diakses dari https://www.bps.go.id/id/pressrelease/2023/11/06/2002/tingkat-pengangguran-terbuka--tpt--sebesar-5-32-persen-dan-rata-rata-upah-buruh-sebesar-3-18-juta-rupiah-per-bulan.html .</p>
<p>[2] R. Pramudjasi, Juliansyah, D. Lestari. (2019). Effect of population and education and wages on unemployment in paser regency. Journal of Faculty of Economics and Business Universitas Mulawarman. Diakses dari https://journal.feb.unmul.ac.id/index.php/KINERJA/article/download/5284/472.</p>
<p>[3] S.A.E. Sari, F.W.Pangestuty. (2020). Analisis Pengaruh jumlah Penduduk, Tingkat Pendidikan, dan Produk Domestik Regional Bruto Terhadap Tingkat Pengangguran Terbuka di Provinsi Jawa Timur Tahun 2017-2020. Journal of Developement Econoic and Social Studies. Diakses dari https://jdess.ub.ac.id/index.php/jdess/article/download/78/57/373.</p>
<p>[4] B. Jepchumba. (2020). Getting started with using Visual Machine Learning Tools for building your Machine Learning Models. Microsoft Community Hub. Diakses dari https://techcommunity.microsoft.com/t5/educator-developer-blog/getting-started-with-using-visual-machine-learning-tools-for/ba-p/3578397.</p>
<p>[5] Google Machine Learning Developers. Normalization. Diakses dari https://developers.google.com/machine-learning/data-prep/transform/normalization#:~:text=The%20goal%20of%20normalization%20is,training%20stability%20of%20the%20model.</p>
<h2 id="daftar-pustaka"><em><strong>Daftar Pustaka</strong></em></h2>
<p>[1] P. Schmitt, J. Mandel, M. Guedj. (2015). A Comparison of Six Methods for Missing Data Imputation. Journal of Biometrics and Biostatics. DOI: 10.472/2155-6180.1000224. Diakses dari https://www.hilarispublisher.com/open-access/a-comparison-of-six-methods-for-missing-data-imputation-2155-6180-1000224.pdf.</p>
<p>[2] Kleindessner, M., Awasthi, P., &amp; Morgenstern, J. (2019). Fair k-Center Clustering for Data Summarization. International Conference on Machine Learning. Diakses dari https://www.semanticscholar.org/paper/Fair-k-Center-Clustering-for-Data-Summarization-Kleindessner-Awasthi/9c26bbf34bdab544a000038d628a8fb232d60cb6.</p>
<p>[3] Goutte, C., &amp; Gaussier, E. (2005). A Probabilistic Interpretation of Precision, Recall and F-Score, with Implication for Evaluation. Advances in Information Retrieval, 345–359. Diakses dari doi:10.1007/978-3-540-31865-1_25.</p>
<p>[4] J. Brownlee. (2020). Machine Learing Mastery. Diakses dari https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/ .</p>
<p>[5] Dicoding Academy. Diakses dari https://www.dicoding.com/ .</p>

</body>
</html>
